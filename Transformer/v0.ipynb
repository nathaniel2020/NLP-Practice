{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ],
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 准备数据\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# S: start\n",
    "sentences = [\n",
    "        # enc_input           dec_input         dec_output\n",
    "        ['ich mochte ein bier P', 'S i want a beer .', 'i want a beer . E'],\n",
    "        ['ich mochte ein cola P', 'S i want a coke .', 'i want a coke . E']\n",
    "]\n",
    "\n",
    "# vocabulary dictionary\n",
    "src_vocab = {'P' : 0, 'ich' : 1, 'mochte' : 2, 'ein' : 3, 'bier' : 4, 'cola' : 5} # Source language\n",
    "src_vocab_size = len(src_vocab)\n",
    "\n",
    "tgt_vocab = {'P' : 0, 'i' : 1, 'want' : 2, 'a' : 3, 'beer' : 4, 'coke' : 5, 'S' : 6, 'E' : 7, '.' : 8} # target language\n",
    "idx_to_word = {i: word for i, word in enumerate(tgt_vocab)} # to translate the indexes sequence to words sequence\n",
    "tgt_vocab_size = len(tgt_vocab)\n",
    "\n",
    "enc_max_len = 5 # the max sequence length of enc_input\n",
    "dec_max_len = 6 # the max sequence length of dec_input and dec_output"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def make_dataset(sentences):\n",
    "    '''\n",
    "    transform the word sentences into indexes tensor\n",
    "    :param sentences: list of list(have three elements: enc_input, dec_input, dec_output) of string\n",
    "    :return: Three torch.LongTensor elements: enc_inputs, dec_inputs, dec_outputs\n",
    "            size: (batch_size, max_len(enc or dec))\n",
    "    '''\n",
    "    enc_inputs, dec_inputs, dec_outputs = [], [], []\n",
    "    for i in range(len(sentences)):\n",
    "        enc_input = [src_vocab[word] for word in sentences[i][0].split()]\n",
    "        dec_input = [tgt_vocab[word] for word in sentences[i][1].split()]\n",
    "        dec_output = [tgt_vocab[word] for word in sentences[i][2].split()]\n",
    "\n",
    "        enc_inputs.append(enc_input)\n",
    "        dec_inputs.append(dec_input)\n",
    "        dec_outputs.append(dec_output)\n",
    "\n",
    "    return torch.LongTensor(enc_inputs), torch.LongTensor(dec_inputs), torch.LongTensor(dec_outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "enc_inputs, dec_inputs, dec_outputs = make_dataset(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "class Dataset(Data.Dataset):\n",
    "    '''\n",
    "    construct our own dataset\n",
    "    '''\n",
    "    def __init__(self, enc_inputs, dec_inputs, dec_outputs):\n",
    "        super(Dataset, self).__init__()\n",
    "        self.enc_inputs = enc_inputs\n",
    "        self.dec_inputs = dec_inputs\n",
    "        self.dec_outputs = dec_outputs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.enc_inputs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.enc_inputs[idx], self.dec_inputs[idx], self.dec_outputs[idx]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "data_loader = Data.DataLoader(Dataset(enc_inputs, dec_inputs, dec_outputs))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4, 8, 7]])\n",
      "['i', 'want', 'a', 'beer', '.', 'E']\n",
      "tensor([[1, 2, 3, 5, 8, 7]])\n",
      "['i', 'want', 'a', 'coke', '.', 'E']\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "for enc_input, dec_input, dec_output in data_loader:\n",
    "    print(dec_output)\n",
    "    # dec_output size: (batch_size=1, dec_max_len=6)\n",
    "    # use sequeeze() to make dec_output into one dimension, size:(dec_max_len=6)\n",
    "    # idx.item() to take out the value(int)\n",
    "    print([idx_to_word[idx.item()] for idx in dec_output.squeeze(dim=0)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 定义模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Parameter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "d_k = d_v = 64 # (d_model / n_heads) the dimension of Q, K must be equal, and V does not have limit\n",
    "n_heads = 8 # multi head attention\n",
    "d_ff = 2048 # the dimension of feed forward\n",
    "n_layers = 6 # number of encoder layer and decoder layer\n",
    "lr=1e-3 # learning rate\n",
    "momentum=0.99 # parameters of SGD\n",
    "EPOCH = 20"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Positional Encoding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        '''\n",
    "\n",
    "        :param d_model: the dimension of one word embedding\n",
    "        :param dropout:\n",
    "        :param max_len: the maximum length of input data, just need bigger than the batch_size\n",
    "        '''\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.ones(max_len, d_model)\n",
    "        pe = pe * torch.arange(max_len).reshape(-1, 1) # to make every row's value is there index\n",
    "        pe = pe / torch.pow(10000, torch.arange(d_model) * 2 / d_model)\n",
    "        pe[:, 0::2] = torch.sin(pe)[:, 0::2] # the dimension index is even\n",
    "        pe[:, 1::2] = torch.cos(pe)[:, 1::2] # odd\n",
    "        # pe: (max_len, 1, d_model)\n",
    "        pe = pe.unsqueeze(1) # because the input x has three dimension (batch_size, seq_len, d_model)\n",
    "        self.register_buffer('pe', pe) # register the tensor into buffer that it will not be update by optimizer\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        seq_len is equal enc_max_len or dec_max_len\n",
    "        :param x: (batch_size, seq_len, d_model) the word embedding of a batch\n",
    "        :return: (batch_size, seq_len, d_model) which add the positional information\n",
    "        '''\n",
    "        x = x + self.pe[:x.shape[0],: ,:]\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 5, 512])"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "test_input = enc_input\n",
    "test_emb_layer = nn.Embedding(src_vocab_size, d_model)\n",
    "test_emb = test_emb_layer(test_input)\n",
    "test_pos_layer = PositionalEncoding(d_model=d_model)\n",
    "test_pos = test_pos_layer(test_emb)\n",
    "test_pos.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Padding Mask\n",
    "To deal with the non-fixed length sequence\n",
    "We need use padding to fill the short sequence\n",
    "and the padding mark does not supply information\n",
    "so we use matrix to mask this padding\n",
    "and after softmax the corresponding probability is 0\n",
    "\n",
    "不定长文本需要截断或者填充，而填充标记不提供任何有用的信息，\n",
    "所以使用矩阵来把这部分遮盖起来，使得softmax计算后其对应的概率为0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "def get_padding_mask(seq_q, seq_k):\n",
    "    '''\n",
    "    seq_q is the query\n",
    "    and we want to know the influence to seq_q of each element in sek_k\n",
    "    so is the element in seq_k is padding mark, it can not supply information\n",
    "    thus we need to change the score with a minim value before softmax\n",
    "    This function is just a mark that assign the position\n",
    "    which the value is True should be modify with a minim value\n",
    "\n",
    "    seq_q and seq_k is the raw input(haven't through embedding layer)\n",
    "    :param seq_q: (batch_size, q_seq_len, d_model)\n",
    "    :param seq_k: (batch_size, k_seq_len, d_model) k_seq_len may be different with q_seq_len\n",
    "    :return: (batch_size, q_seq_len, k_seq_len)\n",
    "    '''\n",
    "    batch_size, q_len = seq_q.size()\n",
    "    _, k_len = seq_k.size()\n",
    "    mask = seq_k.data.eq(0)\n",
    "    # expand do not allocate new memory, it just creates a new view\n",
    "    return mask.expand(batch_size, q_len, k_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[False, False, False, False,  True],\n         [False, False, False, False,  True],\n         [False, False, False, False,  True],\n         [False, False, False, False,  True],\n         [False, False, False, False,  True]]])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "\n",
    "test_seq_q, test_seq_k = enc_input, enc_input\n",
    "test_padding_mask = get_padding_mask(test_seq_q, test_seq_k)\n",
    "test_padding_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Sequence Mask\n",
    "\n",
    "Prevent disclosure of information"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "def get_sequence_mask(seq):\n",
    "    '''\n",
    "    only used in decoder, the position which value equal 1 should be masked\n",
    "    :param seq: (batch_size, seq_len=dec_max_len)\n",
    "    :return: (batch_size, seq_len=dec_max_len, seq_len=dec_max_len) a lower triangular matrix\n",
    "    '''\n",
    "    mask_size = seq.size()[0], seq.size()[1], seq.size()[1]\n",
    "    sequence_mask = torch.triu(torch.ones(mask_size), diagonal=1) # Upper triangular matrix\n",
    "    return sequence_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0., 1., 1., 1., 1., 1.],\n         [0., 0., 1., 1., 1., 1.],\n         [0., 0., 0., 1., 1., 1.],\n         [0., 0., 0., 0., 1., 1.],\n         [0., 0., 0., 0., 0., 1.],\n         [0., 0., 0., 0., 0., 0.]]])"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "test_sequence_mask = get_sequence_mask(dec_input)\n",
    "test_sequence_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 Scaled Dot Product Attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask):\n",
    "    '''\n",
    "    in transformer, it has thress attention operation\n",
    "    two of them are self-attention that q_seq_len equal to k_seq_len\n",
    "    one of them is attention(the output of encoder as Q and V, the output of front part decoder as Q)\n",
    "    :param Q: torch.tensor (batch_size, n_heads, q_seq_len, d_k)\n",
    "    :param K: torch.tensor (batch_size, n_heads, k_seq_len, d_k)\n",
    "    :param V: torch.tensor (batch_size, n_heads, v_seq_len=k_seq_len, d_v)\n",
    "    :param mask: torch.tensor (batch_size, n_heads, q_seq_len, k_seq_len)\n",
    "    :return: torch.tensor (batch_size, n_heads, q_seq_len, d_v)\n",
    "            torch.tensor (batch_size, n_heads. q_seq_len, k_seq_len)\n",
    "    '''\n",
    "    softmax = nn.Softmax(dim=-1) # we need do softmax in d_k's dimension(the row and is the last dimension, also can use 3 int this proejct)\n",
    "\n",
    "    sources = torch.matmul(Q, K.transpose(-1,-2)) / np.sqrt(d_k) # exchange the last dimension of K\n",
    "    sources.masked_fill(mask, -1e9) # mask operation\n",
    "    atten_source = softmax(sources)\n",
    "    return torch.matmul(atten_source, V), atten_source"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.6 MultiHeadAttention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.W_Q = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, n_heads * d_k, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, n_heads * d_v, bias=False)\n",
    "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
    "\n",
    "    def forward(self, Q, K, V, mask):\n",
    "        '''\n",
    "\n",
    "        :param Q: torch.tensor (batch_size, q_seq_len, d_model)\n",
    "        :param K: torch.tensor (batch_size, k_seq_len, d_model)\n",
    "        :param V: torch.tensor (batch_size, v_seq_len=k_seq_len, d_model)\n",
    "        :param mask: torch.tensor (batch_size, q_seq_len, k_seq_len)\n",
    "        :return: context: (batch_size, q_seq_len, d_model)\n",
    "                atten: (batch_size, n_heads, q_seq_len, k_seq_len)\n",
    "        '''\n",
    "        batch_size, q_seq_len, k_seq_len = mask.size()\n",
    "        residual = Q # residual connection\n",
    "        # through the liner layer, do attention in a small(d_k, d_v dimension) projection space\n",
    "        # self.W_Q(Q): (batch_size, q_seq_len, n_heads*d_model)\n",
    "        Q = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2) # Q: (batch_size, n_heads, q_seq_len, d_k)\n",
    "        K = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "        V = self.W_V(V).view(batch_size, -1, n_heads, d_k).transpose(1,2)\n",
    "\n",
    "        mask = mask.unsqueeze(1).expand(batch_size, n_heads, q_seq_len, k_seq_len)\n",
    "        # context: (batch_size, n_heads, q_seq_len, d_v)\n",
    "        # atten: (batch_size, n_heads, q_seq_len, k_seq_len)\n",
    "        context, atten = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # project the vector to the original dimension size(d_model)\n",
    "        # if do not use contiguous(), also can use reshape function to replace view\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        context = self.fc(context) # (batch_size, q_seq_len, d_model)\n",
    "        return nn.LayerNorm(d_model)(context + residual), atten # residual connection and layernorm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.7 FeedForward Layer\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=False)\n",
    "        )\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "\n",
    "        :param input: torch.tensor (batch_size, q_seq_len, d_model) the output of multi head attention\n",
    "        :return: (batch_size, q_seq_len, d_model)\n",
    "        '''\n",
    "        residual = input\n",
    "        output = self.fc(input) # (batch_size, q_seq_len, d_model)\n",
    "        return nn.LayerNorm(d_model)(output + residual)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.8 Encoder Layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # attention in encoder is self attention\n",
    "        self.enc_multi_self_atten = MultiHeadAttention()\n",
    "        self.feed_forward = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, enc_inputs, mask):\n",
    "        '''\n",
    "        the mask is self attention's mask, obtain from the get_padding_mask() function\n",
    "        :param enc_inputs: torch.tensor (batch_size, enc_max_len, d_model)\n",
    "        :param mask: torch.tensor (batch_size, q_seq_len=enc_max_len, k_seq_len=enc_max_len)\n",
    "        :return:\n",
    "        '''\n",
    "        # context: (batch_size, q_seq_len=enc_max_len, d_model)\n",
    "        # atten: (batch_size, n_heads, q_seq_len=enc_max_len, k_seq_len=enc_max_len)\n",
    "        context, atten = self.enc_multi_self_atten(enc_inputs, enc_inputs, enc_inputs, mask)\n",
    "        context = self.feed_forward(context) # (batch_size, q_seq_len=enc_max_len, d_model)\n",
    "        return context, atten"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.9 Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        # the size of corpora, the dimension of the embedding\n",
    "        self.emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs):\n",
    "        '''\n",
    "\n",
    "        :param enc_inputs: torch.tensor (batch_size, enc_max_len)\n",
    "        :return: enc_outputs: torch.tensor (batch_size, enc_max_len, d_model)\n",
    "                enc_self_attn: list(length = n_layers) of torch.tensor(size: (batch_size, q_seq_len=enc_max_len, k_seq_len=enc_max_len))\n",
    "        '''\n",
    "        enc_outputs = self.emb(enc_inputs) # (batch_size, enc_max_len, d_model)\n",
    "        enc_outputs = self.pos(enc_outputs) # (batch_size, enc_max_len, d_model)\n",
    "\n",
    "        enc_self_attn = [] # a list that store the each encoder layer's attention result\n",
    "        mask = get_padding_mask(enc_inputs, enc_inputs) # encoder should use padding mask\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            # enc_outputs: (batch_size, enc_max_len, d_model)\n",
    "            # atten: (batch_size, q_seq_len=enc_max_len, k_seq_len=enc_max_len)\n",
    "            enc_outputs, atten = encoder_layer(enc_outputs, mask)\n",
    "            enc_self_attn.append(atten)\n",
    "        return enc_outputs, enc_self_attn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.10 Decoder Layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_multi_self_atten = MultiHeadAttention()\n",
    "        self.dec_multi_enc_atten = MultiHeadAttention()\n",
    "        self.feed_forward = PoswiseFeedForwardNet()\n",
    "\n",
    "    def forward(self, dec_inputs, enc_outputs, dec_self_atten_mask, dec_enc_atten_mask):\n",
    "        '''\n",
    "\n",
    "        :param dec_inputs: torch.tensor (batch_size, q_seq_len=dec_max_len, d_model)\n",
    "        :param enc_outputs: torch.tensor (batch_size, k_seq_len=enc_max_len, d_model)\n",
    "        :param dec_self_atten_mask: torch.tensor (batch_size, q_seq_len=dec_max_len, k_seq_len=dec_max_len)\n",
    "        :param dec_enc_atten_mask: torch.tensor (batch_size, q_seq_len=dec_max_len, k_seq_len=enc_max_len)\n",
    "        :return:\n",
    "        '''\n",
    "        # dec_outputs: (batch_size, q_seq_len=dec_max_len, d_model)\n",
    "        # dec_self_atten: (batch_size, q_seq_len=dec_max_len, k_seq_len=dec_max_len)\n",
    "        dec_outputs, dec_self_atten = self.dec_multi_self_atten(dec_inputs, dec_inputs, dec_inputs, dec_self_atten_mask)\n",
    "        # dec_outputs: (batch_size, q_seq_len=dec_max_len, d_model)\n",
    "        # dec_enc_atten: (batch_size, q_seq_len=dec_max_len, k_seq_len=enc_max_len)\n",
    "        dec_outputs, dec_enc_atten = self.dec_multi_enc_atten(dec_outputs, enc_outputs, enc_outputs, dec_enc_atten_mask)\n",
    "        dec_outputs = self.feed_forward(dec_outputs) # (batch_size, q_seq_len=dec_max_len, d_model)\n",
    "        return dec_outputs, dec_self_atten, dec_enc_atten"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.11 Decoder"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs):\n",
    "        '''\n",
    "\n",
    "        :param dec_inputs: (batch_size, dec_max_len)\n",
    "        :param enc_inputs: (batch_size, enc_max_len)\n",
    "        :param enc_outputs: (batch_size, enc_max_len, d_model)\n",
    "        :return: torch.tensor (batch_size, dec_max_len, d_model)\n",
    "                list(length=n_layers) of torch.tensor((batch_size, dec_max_len, dec_max_len))\n",
    "                list(length=n_layers) of torch.tensor((batch_size, dec_max_len, enc_max_len))\n",
    "        '''\n",
    "\n",
    "        dec_outputs = self.emb(dec_inputs)\n",
    "        dec_outputs = self.pos(dec_outputs)\n",
    "\n",
    "        # decoder self attention should prevent disclosure of information\n",
    "        # thus we need combine the two mask together\n",
    "        dec_self_atten_seq_mask = get_sequence_mask(dec_inputs) # 1 or 0 (batch_size, dec_max_len, dec_max_len)\n",
    "        dec_self_atten_pad_mask = get_padding_mask(dec_inputs, dec_inputs) # True or False (batch_size, dec_max_len, dec_max_len)\n",
    "        dec_self_atten_mask = torch.gt((dec_self_atten_pad_mask + dec_self_atten_seq_mask), 0) # value > 0 is True\n",
    "        # dec_outputs as the query\n",
    "        dec_enc_atten_mask = get_padding_mask(dec_inputs, enc_inputs) # (batch_size, dec_max_len, enc_max_len)\n",
    "\n",
    "        dec_self_attns, dec_enc_attns = [], []\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            dec_outputs, dec_self_atten, dec_enc_atten = decoder_layer(dec_outputs, enc_outputs, dec_self_atten_mask, dec_enc_atten_mask)\n",
    "            dec_self_attns.append(dec_self_atten)\n",
    "            dec_enc_attns.append(dec_enc_atten)\n",
    "\n",
    "        return dec_outputs, dec_self_attns, dec_enc_attns\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.12 Transformer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        # use CrossEntropyLoss so in the last layer we don't need to do softamx\n",
    "        self.liner = nn.Linear(d_model, tgt_vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, enc_inputs, dec_inputs):\n",
    "        '''\n",
    "\n",
    "        :param enc_inputs: torch.tensor (batch_size, enc_max_len)\n",
    "        :param dec_inputs: torch.tensor (batch_size, dec_max_len)\n",
    "        :return: outputs: torch.tensor (batch_size * dec_max_len, tgt_vocab_size)\n",
    "                enc_self_attn: list(length = n_layers) of torch.tensor(size: (batch_size, q_seq_len=enc_max_len, k_seq_len=enc_max_len))\n",
    "                dec_self_atten: list(length=n_layers) of torch.tensor((batch_size, dec_max_len, dec_max_len))\n",
    "                dec_enc_atten: list(length=n_layers) of torch.tensor((batch_size, dec_max_len, enc_max_len))\n",
    "        '''\n",
    "        enc_outputs, enc_self_attn = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_self_atten, dec_enc_atten = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        outputs = self.liner(dec_outputs) # (batch_size, dec_max_len, tgt_vocab_size)\n",
    "        outputs = outputs.view(-1, outputs.size(-1)) # (batch_size * dec_max_len, tgt_vocab_size)\n",
    "        return outputs, enc_self_attn, dec_self_atten, dec_enc_atten"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.13 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss = 2.257706\n",
      "Epoch: 0002 loss = 1.791153\n",
      "Epoch: 0003 loss = 1.327324\n",
      "Epoch: 0004 loss = 0.915619\n",
      "Epoch: 0005 loss = 0.519997\n",
      "Epoch: 0006 loss = 0.247172\n",
      "Epoch: 0007 loss = 0.124045\n",
      "Epoch: 0008 loss = 0.094626\n",
      "Epoch: 0009 loss = 0.084549\n",
      "Epoch: 0010 loss = 0.065260\n",
      "Epoch: 0011 loss = 0.049262\n",
      "Epoch: 0012 loss = 0.043676\n",
      "Epoch: 0013 loss = 0.039182\n",
      "Epoch: 0014 loss = 0.029716\n",
      "Epoch: 0015 loss = 0.019470\n",
      "Epoch: 0016 loss = 0.013400\n",
      "Epoch: 0017 loss = 0.011721\n",
      "Epoch: 0018 loss = 0.012308\n",
      "Epoch: 0019 loss = 0.012897\n",
      "Epoch: 0020 loss = 0.011822\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    loss_list = []\n",
    "    for enc_inputs, dec_inputs, dec_outputs in data_loader:\n",
    "        outputs, *_ = model(enc_inputs, dec_inputs)\n",
    "        loss = criterion(outputs, dec_outputs.view(-1))\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_avg = sum(loss_list) / len(loss_list)\n",
    "    print('Epoch:', '{:0>4d}'.format(epoch + 1), 'loss =', '{:.6f}'.format(loss_avg))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.14 Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "def translate(model, enc_input, start_symbol):\n",
    "    outputs = model(enc_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}